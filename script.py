import os
import requests
import psycopg2
from dotenv import load_dotenv
from supabase import create_client, Client

# --- Configuration ---
load_dotenv()
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_KEY")
APP_SCRIPT_URL = os.getenv("APPS_SCRIPT_URL")
DB_CONNECTION_STRING = os.getenv("DATABASE_URI")

def get_value_type_level(value):
    """Determines the type level for a single value: 0 for INT, 1 for FLOAT, 2 for TEXT."""
    if value is None or str(value).strip() == "":
        return -1  # Ignore empty values

    try:
        # Check if it can be represented as a whole number (e.g., "10" or 10.0)
        if float(str(value)) == int(float(str(value))):
            return 0  # Treat as BIGINT
    except (ValueError, TypeError):
        # Cannot be converted to a float at all, so it must be TEXT
        return 2

    try:
        # If the above passed, check if it's a valid float
        float(str(value))
        return 1  # Treat as FLOAT8
    except (ValueError, TypeError):
        return 2  # Fallback to TEXT

def get_column_types(rows):
    """Scans all rows to determine the most accommodating data type for each column."""
    if not rows:
        return {}
    headers = list(rows[0].keys())
    type_hierarchy = ['BIGINT', 'FLOAT8', 'TEXT']
    # Start all columns at the most restrictive level (BIGINT)
    column_levels = {h: 0 for h in headers}

    for row in rows:
        for col_name, value in row.items():
            if col_name not in column_levels:
                continue
            
            # Determine the type level of the current value
            value_level = get_value_type_level(value)
            
            # Upgrade the column's type level if the current value is more complex
            if value_level > column_levels[col_name]:
                column_levels[col_name] = value_level
                
    # Convert the final levels back into SQL type names
    final_column_types = {h: type_hierarchy[level] for h, level in column_levels.items()}
    return final_column_types

def sync_to_db():
    """
    Fetches data from a Google Apps Script endpoint and synchronizes it 
    to a PostgreSQL database (Supabase), recreating tables for a fresh sync.
    """
    if not all([SUPABASE_URL, SUPABASE_KEY, APP_SCRIPT_URL, "[YOUR-PASSWORD]" not in DB_CONNECTION_STRING]):
        print("‚ùå Error: Missing environment variables or placeholder password. Please check your .env file and script.")
        return

    try:
        print("üöÄ Fetching data from Google Apps Script...")
        response = requests.get(APP_SCRIPT_URL)
        response.raise_for_status()
        sheets_data = response.json()
        print(f"‚úÖ Successfully fetched data for {len(sheets_data)} sheet(s).")
        
        supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
        conn = psycopg2.connect(DB_CONNECTION_STRING)
        cursor = conn.cursor()

    except Exception as e:
        print(f"‚ùå An error occurred during setup: {e}")
        return

    for table_name, rows in sheets_data.items():
        sanitized_table_name = "".join(c if c.isalnum() else '_' for c in table_name)
        print(f"\n--- Processing table: {sanitized_table_name} ---")

        if not rows:
            print("-  Skipping empty sheet.")
            continue

        print("-  Scanning all rows to determine final schema...")
        column_definitions = get_column_types(rows)

        try:
            print(f"-  Dropping existing table '{sanitized_table_name}' to ensure a fresh start...")
            cursor.execute(f'DROP TABLE IF EXISTS "{sanitized_table_name}";')

            print(f"-  Defining schema for '{sanitized_table_name}'...")
            create_table_sql = f'CREATE TABLE "{sanitized_table_name}" ('
            create_table_sql += '"id" BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,'
            
            cols_sql_parts = []
            for col_name, sql_type in column_definitions.items():
                sanitized_col_name = "".join(c if c.isalnum() else '_' for c in col_name)
                cols_sql_parts.append(f'"{sanitized_col_name}" {sql_type}')
            
            create_table_sql += ", ".join(cols_sql_parts) + ');'
            
            cursor.execute(create_table_sql)
            conn.commit()
            
            print("-  Reloading Supabase schema cache...")
            cursor.execute("NOTIFY pgrst, 'reload schema';")
            
            print(f"‚úÖ Table '{sanitized_table_name}' is ready.")

        except Exception as e:
            print(f"‚ùå Error creating/dropping table '{sanitized_table_name}': {e}")
            conn.rollback()
            continue

        try:
            print(f"-  Inserting {len(rows)} rows...")
            
            clean_rows = []
            for row in rows:
                clean_row = {}
                for key, val in row.items():
                    if key:
                        sanitized_key = "".join(c if c.isalnum() else '_' for c in key)
                        # The original script relies on upsert which will perform a type cast
                        # for the column. To maintain original logic, we clean the keys
                        # but let Supabase handle the type conversion on insert/upsert.
                        clean_row[sanitized_key] = None if val is None or str(val).strip() == "" else val
                clean_rows.append(clean_row)

            # Supabase upsert requires a primary key, which is the 'id' we created.
            # It will insert new rows and update existing ones (though here, all are new
            # due to the preceding DROP TABLE).
            supabase.table(sanitized_table_name).upsert(clean_rows).execute()
            print(f"‚úÖ Successfully inserted/updated data for '{sanitized_table_name}'.")

        except Exception as e:
            print(f"‚ùå Error inserting data into '{sanitized_table_name}': {e}")
            continue
            
    cursor.close()
    conn.close()
    print("\nüéâ All sheets processed. Sync complete!")

# --- Main Script Execution Block ---
if __name__ == "__main__":
    sync_to_db()